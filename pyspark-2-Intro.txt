SparkContext:
	Main entry point for Spark functionality. 
	A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. 
	is an entry point to the PySpark functionality that is used to communicate with the cluster and to create an RDD, accumulator, and broadcast variables. 
	The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs, and know what resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application.
	You can stop the SparkContext by calling the stop() method. As explained above you can have only one SparkContext per JVM. If you wanted to create another, you need to shutdown it first by using stop() method and create a new SparkContext.


SparkSession:
	is a combined class for all different contexts
	SparkSession also includes all the APIs available in different contexts:
		SparkContext,
		SQLContext,
		StreamingContext,
		HiveContext.
	Creating a SparkSession creates a SparkContext internally and exposes the sparkContext variable to use.	

SparkConf:
	Configuration for a Spark application. Used to set various Spark parameters as key-value pairs.

Example Code:
	import pyspark
	from pyspark.sql import SparkSession
	spark = SparkSession.builder.master("local").appName('myFirstApp').getOrCreate()
	# Set Config
	spark.conf.set("spark.executor.memory", "5g")
	# Get a Spark Config
	partions = spark.conf.get("spark.sql.shuffle.partitions")
	print(spark.sparkContext)
	print("Spark App Name : "+ spark.sparkContext.appName)

	The appName parameter is a name for your application to show on the cluster UI. 
	master is a Spark, Mesos or YARN cluster URL, or a special “local” string to run in local mode. 


Launching Applications with spark-submit: 
	[https://spark.apache.org/docs/latest/submitting-applications.html#bundling-your-applications-dependencies]

	./bin/spark-submit \
	  --verbose \
	  --deploy-mode <deploy-mode> \
	  --conf <key>=<value> \
	  --driver-memory 8g \
	  --executor-memory 16g \
	  --executor-cores 2  \
      --py-files file1.py,file2.py,file3.zip
      <python spark app>.py
	  [application-arguments]

	For Python, you can use the --py-files argument of spark-submit to add .py, .zip or .egg files to be distributed with your application. If you depend on multiple Python files we recommend packaging them into a .zip or .egg. For third-party Python dependencies, see Python 

	--master: The master URL for the cluster (e.g. spark://23.195.26.187:7077)
	--deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client) †
	--conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown). Multiple configurations should be passed as separate arguments. (e.g. --conf <key>=<value> --conf <key2>=<value2>)

Running Spark local cluster:
	1- Run spark local cluster
	./sbin/start-master.sh

	2- Pick Master URL
	INFO Master: Starting Spark master at spark://prakhars-MacBook-Air.local:7077

	3- Run Worker
	./sbin/start-worker.sh spark://prakhars-MacBook-Air.local:7077
	
